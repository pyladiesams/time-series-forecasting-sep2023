{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: [Kaggle](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption?select=PJMW_hourly.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EnerNed Hourly Energy Consumption Data Scenario\n",
    "\n",
    "EnerNed is a regional transmission organization in the United States. It is part of the Eastern Interconnection grid operating an electric transmission system in various states.\n",
    "\n",
    "The company noticed that sometimes it is hard for them to assume what the future demand for power will be. They would like to have a predictive model that would help them in being ready for the increase in demand as well as regulating the transmission in the system once the demand is lower. They would like to also understand how the trends are shaped over time.\n",
    "\n",
    "The hourly power consumption data comes from EnerNed's website and are in megawatts (MW). Follow the steps below to create a time series model. Please fill the code in the cells where indicated. Don’t worry about the time. If you don’t manage to finish all the exercises, you can always do it at home and compare your answers with the solutions provided in our repository.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run just once and then comment it out\n",
    "%cd ..\n",
    "%conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda activate pyladies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.regression import *\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is loading the data. Then we are going to do some initial operations like changing the datatypes, sorting it and setting the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"PJMW_hourly.csv\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 rows of th dataset\n",
    "raw_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below saves names of the date and target columns. It will facilitate the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col = \"Datetime\"\n",
    "target = \"PJMW_MW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform columns to appropriate data types. \n",
    "Use the following [link](https://www.geeksforgeeks.org/change-data-type-for-one-or-more-columns-in-pandas-dataframe/) in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the date column datetime type \n",
    "raw_data[date_col] =\n",
    "\n",
    "# Make the target column \"float32\"\n",
    "raw_data[target] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort Dataframe and set index. Use the following [link](https://www.geeksforgeeks.org/how-to-sort-pandas-dataframe/) & [link](https://www.geeksforgeeks.org/python-pandas-dataframe-set_index/) in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by the date column\n",
    "raw_data = raw_data.\n",
    "\n",
    "# Set the date columns as an index\n",
    "raw_data = raw_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ALWAYS THINK OF THE PROBLEM BEFORE CREATING FEATURES!__\n",
    "\n",
    "In this case, we need to predict hourly consumption of energy for 1 year ahead. Therefore, we will not be able to create features such as lag_1_day, lag_2_months and so forth. Instead, we must create features such as lag_1_year, lag_14_months and so forth. Always think of data availability. You cannot create lag/rolling features in the future, except for those that go back at least for the same time frame as the forecasting horizon, in this case 1 year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that can be done is to incorporate other features such as GDP, PCI and so forth, but when we are forecasting out of sample, therefore when we are forecasting the future, we will need the __forecasts for those features__, because obviously we will not have them at the origin time when creating the future forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is stil possible to use lag and rolling windows features that have a shorter time frame than the forecasting horizon, but this will mean that we will train a model with say 10 features, but at the origin we will make forecasts using only 5 features, excluding lags and rolling windows. This can have several implications:\n",
    "\n",
    "- __Out-of-Sample Performance__: The model's performance on the test data (and potentially on out-of-sample data within the testing period) might be reasonably accurate due to the availability of lag and rolling window features during evaluation. However, this performance doesn't guarantee how well the model will generalize to future periods when those features cannot be created.\n",
    "- __Potential Performance Drop-off__: The model's performance may degrade when forecasting into the future without the lag and rolling window features. This is because the model has learned to rely on those specific features to make predictions, and without them, it might struggle to capture certain patterns or trends.\n",
    "- __Reduced Feature Space__: In the real-world scenario, if you cannot create lag and rolling window features, you'll be limited to using only the features available at that time (e.g., the 5 features you mentioned). This means the model might not have access to all the information it was trained on, which can limit its forecasting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataframe\n",
    "fe_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple features function and then apply it on our dataset. Use the following [link](https://www.geeksforgeeks.org/python-datetime-datetime-class/) & [link](https://www.geeksforgeeks.org/isocalendar-function-of-datetime-date-class-in-python/) in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dt(df):\n",
    "    \"\"\"\n",
    "    Extracts several datetime objects from a datetime index.\n",
    "    \"\"\"\n",
    "    df['hour'] = df.index.hour                           \n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['quarter'] = df.index.quarter\n",
    "    df['month'] = df.index.month           # Month\n",
    "    df['year'] = df.index.          # Year \n",
    "    df['dayofyear'] = df.index.dayofyear   # Day of the year\n",
    "    df['dayofmonth'] = df.       # Day of the month\n",
    "    df['weekofyear'] =  # Week of the year based on the isocalendaryear\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to create the dataset (input df = fe_data)\n",
    "\n",
    "fe_data = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(fe_data['PJMW_MW'])\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('PJMW_MW')\n",
    "plt.title('Plot of PJMW_MW')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could already train and test a model with only these features, as they can be available in the future, because they simply extract datetime information from dates. Instead we are going to add lags as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lags(df, target, lags_dict):\n",
    "    \"\"\"\n",
    "    Creates a mapping between index and target that is used to create lags based on\n",
    "    a dictionary of lag keys and values.\n",
    "    \"\"\"\n",
    "    target_map = df[target].to_dict()\n",
    "    for lag, lag_days in lags_dict.items():\n",
    "        df[f'lag_{lag}_year'] = (df.index - pd.Timedelta(lag_days)).map(target_map)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_dict = {\n",
    "    1: \"364 days\",\n",
    "    2: \"728 days\",\n",
    "    3: \"1092 days\"\n",
    "}\n",
    "\n",
    "fe_data = add_lags(fe_data, target, lags_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to remove the index, otherwise pycaret will complain. Use the following [link](https://www.w3schools.com/python/pandas/ref_df_reset_index.asp) in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the index of the dataframe \n",
    "training_data = fe_data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember this is a supervised learning problem, therefore use the [regression module](https://pycaret.readthedocs.io/en/stable/api/regression.html#pycaret.regression.setup) from pycaret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to have names of the features stored as a list. Use the following [link](https://www.geeksforgeeks.org/how-to-drop-one-or-multiple-columns-in-pandas-dataframe/) in case you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the features names and drop the target column from this list \n",
    "\n",
    "features = list()\n",
    "features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a custom cross-validation fold strategy with a gap.\\\n",
    "[Here](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4) you can find an explaination of what cross-validation is and how to use it in time-series. \\\n",
    "Please go to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) to find out more about the module you need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this module TimeSeriesSplit with 5 splits and gap =24 (use the link above for guidance)\n",
    "\n",
    "fold_strategy ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://pycaret.readthedocs.io/en/stable/api/regression.html#pycaret.regression.setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_setup_params = {\n",
    "    \"data\": training_data,\n",
    "    \"target\": target,\n",
    "    \"train_size\": 0.7, # Proportion of the dataset to be used for training and validation. Should be between 0.0 and 1.0.\n",
    "    \"numeric_features\": features, # in this example all features are numeric. pycaret autoamtically detects type of features, but it can sometimes fail\n",
    "    \"imputation_type\": \"simple\", #The type of imputation to use. Can be either ‘simple’ or ‘iterative’. If None, no imputation of missing values is performed.\n",
    "    \"numeric_imputation\": \"median\", # mean/median do not lead to data leakage; due to a bug in pycaret library, dropping missing values results in a difference between features and target column lengths\n",
    "    \"normalize\": True, #When set to True, it transforms the features by scaling them to a given range.\n",
    "    \"normalize_method\": \"zscore\", # Defines the method for scaling.\n",
    "    \"data_split_shuffle\": False, # consider time dimension, therefore set to False\n",
    "    \"fold_strategy\": fold_strategy, #defined in previous step\n",
    "    \"fold\": 5,\n",
    "    \"fold_shuffle\": False,\n",
    "    \"log_experiment\": True,\n",
    "    \"experiment_name\": \"Baseline\",\n",
    "    \"log_plots\": True, \n",
    "    # \"profile\": True, # no need to run it every time\n",
    "    # \"log_profile\": True,\n",
    "    \"session_id\": 420\n",
    "}\n",
    "\n",
    "modeling_setup = setup(\n",
    "    **modeling_setup_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pycaret offers two main ways of creating, training and evaluating models:\n",
    "- compare_models(): will train and evaluate every model installed\n",
    "- create_model(): will train and evaluate a chosen model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try first [create_model()](https://pycaret.readthedocs.io/en/stable/api/regression.html#pycaret.regression.create_model). It will train and evaluate a chosen model, we choose the xgboost algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_params = {\n",
    "    'n_estimators': 10,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "reg = xgb.XGBRegressor(**starting_params)\n",
    "model = create_model(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate several aspects of the model, from the residuals to validation curves (note, it probably freezes the kernel, use it only when needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model)\n",
    "# TODO check which features from the display we want to talk about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Residuals distribution\n",
    "- train set: appear to be normally distributed around zero with relatively low standard deviation; fairly good\n",
    "- test set: appear to be normally distributed around zero, but with a higher standard deviation compared to residuals on train set; model appears unable to capture all underlying relationships in the data\n",
    "\n",
    "2. Homoscedasticity:\n",
    "- train set: given the distirbution of residuals, residuals appear to have a fairly constant variance; good\n",
    "- test set: residuals are not consistently spread as we move along the x-axis\n",
    "\n",
    "3. Outliers:\n",
    "- train/test sets: few outliers, should be removed\n",
    "\n",
    "Actions to take:\n",
    "- remove outliers\n",
    "- tune model using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can interpret the model based on the test/hold-out set using SHAP, to get a better understanding of how each feature impacts the model output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(model, plot=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setup new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a new model with outliers removal and ready for CV hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_setup_params = {\n",
    "    \"data\": training_data,\n",
    "    \"target\": target,\n",
    "    \"remove_outliers\": True,\n",
    "    \"outliers_threshold\": 0.01,\n",
    "    \"train_size\": 0.7,\n",
    "    \"numeric_features\": features,\n",
    "    \"imputation_type\": \"simple\",\n",
    "    \"numeric_imputation\": \"median\",\n",
    "    \"normalize\": True,\n",
    "    \"normalize_method\": \"zscore\",\n",
    "    \"polynomial_features\": True,\n",
    "    \"data_split_shuffle\": False,\n",
    "    \"fold_strategy\": fold_strategy,\n",
    "    \"fold\": 5,\n",
    "    \"fold_shuffle\": False,\n",
    "    \"log_experiment\": True,\n",
    "    \"experiment_name\": \"Candidate 1\",\n",
    "    \"log_plots\": True,\n",
    "    \"session_id\": 420\n",
    "}\n",
    "\n",
    "modeling_setup = setup(\n",
    "    **modeling_setup_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_params = {\n",
    "    'n_estimators': 100,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "reg = xgb.XGBRegressor(**starting_params)\n",
    "model = create_model(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model created on the hold-out set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the trained model is promising. The next thing that can be done is to tune it.\n",
    "<br>\n",
    "There are many methods available [here](https://pycaret.readthedocs.io/en/latest/api/regression.html#pycaret.regression.tune_model).\n",
    "<br>\n",
    "It turns out that __tune-sklearn__ is a faster way to find optimal hyperparameters and it allows to choose different search algorithms, not just random and grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model from before has the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_params = {\n",
    "    \"estimator\": model,\n",
    "    \"n_iter\": 10,\n",
    "    \"early_stopping\": True,\n",
    "    \"optimize\": \"RMSE\",\n",
    "    \"search_library\": \"tune-sklearn\",\n",
    "    \"search_algorithm\": \"hyperopt\",\n",
    "    \"return_train_score\": True\n",
    "}\n",
    "tuned_model = tune_model(**tuned_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the tuned model performance on the hold-out set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "holdout_predictions = predict_model(tuned_model)\n",
    "holdout_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the tuned model performance. To choose a different score to plot, refer to [link](https://www.scikit-yb.org/en/latest/api/model_selection/learning_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tuned_model, plot_kwargs={\"scoring\": \"neg_root_mean_squared_error\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_model(tuned_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyladies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
