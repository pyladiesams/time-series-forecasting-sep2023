{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027cfa65",
   "metadata": {},
   "source": [
    "# Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4001b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-13.0.0-cp38-cp38-win_amd64.whl (24.4 MB)\n",
      "                                              0.0/24.4 MB ? eta -:--:--\n",
      "     -                                        0.7/24.4 MB 14.4 MB/s eta 0:00:02\n",
      "     ---                                      2.2/24.4 MB 23.2 MB/s eta 0:00:01\n",
      "     ------                                   3.9/24.4 MB 27.5 MB/s eta 0:00:01\n",
      "     ---------                                5.9/24.4 MB 31.3 MB/s eta 0:00:01\n",
      "     ------------                             7.9/24.4 MB 33.6 MB/s eta 0:00:01\n",
      "     ----------------                         9.8/24.4 MB 36.6 MB/s eta 0:00:01\n",
      "     ------------------                      11.7/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------                  13.9/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------                15.6/24.4 MB 43.5 MB/s eta 0:00:01\n",
      "     ----------------------------            17.8/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "     -------------------------------         19.8/24.4 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------------------------      21.9/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  24.4/24.4 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 24.4/24.4 MB 31.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\magdalenabrach\\anaconda3\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-13.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\magdalenabrach\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\magdalenabrach\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# %pip install lightgbm\n",
    "# %pip install pandas-profiling\n",
    "# %(pip install --upgrade pandas-profiling)\n",
    "# %pip install ydata-profiling\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5408e1",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25faed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14131041",
   "metadata": {},
   "source": [
    "## Load Sales Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e3297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
    "# A suitable version of pyarrow or fastparquet is required for parquet support.\n",
    "# Trying to import the above resulted in these errors:\n",
    "#  - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
    "#  - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3df2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet \n",
    "sales_dataframe = pd.read_parquet(\"sales_orders_filtered (2).parquet\")\n",
    "\n",
    "# Keep the columns that are useful\n",
    "sales_dataframe = sales_dataframe.drop([\"order_status\", \"ft_sales_orders_identifier\", 'division', 'facility', 'salesperson','warehouse', 'order_date','order_number', 'order_type', 'transaction_reason',\n",
    "       'requested_delivery_date', 'requested_delivery_date_line','line_number',\"highest_status_customer_order_head\", \"ordered_quantity_basic_u_m\", \"requested_delivery_date_obdwdt_\", \"invoiced_quantity_adjusted\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a50751d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_number</th>\n",
       "      <th>item_number</th>\n",
       "      <th>requested_delivery_date_obdwdz_</th>\n",
       "      <th>invoiced_quantity_basic_u_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>182080</td>\n",
       "      <td>11906175</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>182080</td>\n",
       "      <td>11906175</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>182080</td>\n",
       "      <td>11906175</td>\n",
       "      <td>2018-07-04</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>182080</td>\n",
       "      <td>11906175</td>\n",
       "      <td>2018-07-04</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>182080</td>\n",
       "      <td>11906175</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332961</th>\n",
       "      <td>158805</td>\n",
       "      <td>11863535</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332962</th>\n",
       "      <td>239177</td>\n",
       "      <td>11864685</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332963</th>\n",
       "      <td>239177</td>\n",
       "      <td>11864685</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>456.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332964</th>\n",
       "      <td>239177</td>\n",
       "      <td>11864685</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332965</th>\n",
       "      <td>239177</td>\n",
       "      <td>11864685</td>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288348 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_number item_number requested_delivery_date_obdwdz_  \\\n",
       "2               182080    11906175                      2020-03-13   \n",
       "3               182080    11906175                      2020-03-13   \n",
       "4               182080    11906175                      2018-07-04   \n",
       "5               182080    11906175                      2018-07-04   \n",
       "6               182080    11906175                      2018-11-01   \n",
       "...                ...         ...                             ...   \n",
       "332961          158805    11863535                      2021-04-14   \n",
       "332962          239177    11864685                      2021-10-19   \n",
       "332963          239177    11864685                      2021-10-19   \n",
       "332964          239177    11864685                      2021-05-25   \n",
       "332965          239177    11864685                      2021-05-25   \n",
       "\n",
       "        invoiced_quantity_basic_u_m  \n",
       "2                            1500.0  \n",
       "3                            1500.0  \n",
       "4                            1500.0  \n",
       "5                            1500.0  \n",
       "6                             750.0  \n",
       "...                             ...  \n",
       "332961                         70.0  \n",
       "332962                        456.0  \n",
       "332963                        456.0  \n",
       "332964                        600.0  \n",
       "332965                        600.0  \n",
       "\n",
       "[288348 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefd125",
   "metadata": {},
   "source": [
    "# Create time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7811",
   "metadata": {},
   "source": [
    "Explainaition in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea6c323b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m customer_data \u001b[39m=\u001b[39m sales_dataframe[sales_dataframe[\u001b[39m\"\u001b[39m\u001b[39mcustomer_number\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m customer]\n\u001b[0;32m     15\u001b[0m \u001b[39m# Group the data by month and calculate the indicator and sum\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m monthly_grouped \u001b[39m=\u001b[39m customer_data\u001b[39m.\u001b[39;49mgroupby(pd\u001b[39m.\u001b[39;49mGrouper(key\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrequested_delivery_date_obdwdz_\u001b[39;49m\u001b[39m\"\u001b[39;49m, freq\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mM\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39magg(\n\u001b[0;32m     17\u001b[0m     indicator_requested_delivery\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minvoiced_quantity_basic_u_m\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlambda\u001b[39;00m x: \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(x) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)),\n\u001b[0;32m     18\u001b[0m     sum_of_invoiced_quantity_basic_u_m\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minvoiced_quantity_basic_u_m\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[39m# Reindex to the full date range and fill missing values\u001b[39;00m\n\u001b[0;32m     22\u001b[0m monthly_grouped \u001b[39m=\u001b[39m monthly_grouped\u001b[39m.\u001b[39mreindex(date_range, fill_value\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MagdalenaBrach\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MagdalenaBrach\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\MagdalenaBrach\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:786\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39m# a passed-in Grouper, directly convert\u001b[39;00m\n\u001b[0;32m    785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Grouper):\n\u001b[1;32m--> 786\u001b[0m     binner, grouper, obj \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39;49m_get_grouper(obj, validate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    788\u001b[0m         \u001b[39mreturn\u001b[39;00m grouper, \u001b[39mfrozenset\u001b[39m(), obj\n",
      "File \u001b[1;32mc:\\Users\\MagdalenaBrach\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py:1733\u001b[0m, in \u001b[0;36mTimeGrouper._get_grouper\u001b[1;34m(self, obj, validate)\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_grouper\u001b[39m(\u001b[39mself\u001b[39m, obj, validate: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1732\u001b[0m     \u001b[39m# create the resampler and return our binner\u001b[39;00m\n\u001b[1;32m-> 1733\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_resampler(obj)\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m r\u001b[39m.\u001b[39mbinner, r\u001b[39m.\u001b[39mgrouper, r\u001b[39m.\u001b[39mobj\n",
      "File \u001b[1;32mc:\\Users\\MagdalenaBrach\\anaconda3\\lib\\site-packages\\pandas\\core\\resample.py:1725\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[1;34m(self, obj, kind)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(ax, TimedeltaIndex):\n\u001b[0;32m   1721\u001b[0m     \u001b[39mreturn\u001b[39;00m TimedeltaIndexResampler(\n\u001b[0;32m   1722\u001b[0m         obj, groupby\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis, group_keys\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_keys\n\u001b[0;32m   1723\u001b[0m     )\n\u001b[1;32m-> 1725\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1726\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOnly valid with DatetimeIndex, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1727\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTimedeltaIndex or PeriodIndex, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1728\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got an instance of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(ax)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1729\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"
     ]
    }
   ],
   "source": [
    "# Get the min and max dates\n",
    "min_date = sales_dataframe[\"requested_delivery_date_obdwdz_\"].min()\n",
    "max_date = sales_dataframe[\"requested_delivery_date_obdwdz_\"].max()\n",
    "\n",
    "# Create a date range with monthly frequency\n",
    "date_range = pd.date_range(start=min_date, end=max_date, freq=\"M\")\n",
    "\n",
    "# Initialize an empty list to hold the monthly data\n",
    "monthly_data = []\n",
    "\n",
    "# Iterate through each customer\n",
    "for customer in sales_dataframe[\"customer_number\"].unique():\n",
    "    customer_data = sales_dataframe[sales_dataframe[\"customer_number\"] == customer]\n",
    "    \n",
    "    # Group the data by month and calculate the indicator and sum\n",
    "    monthly_grouped = customer_data.groupby(pd.Grouper(key=\"requested_delivery_date_obdwdz_\", freq=\"M\")).agg(\n",
    "        indicator_requested_delivery=(\"invoiced_quantity_basic_u_m\", lambda x: int(len(x) > 0)),\n",
    "        sum_of_invoiced_quantity_basic_u_m=(\"invoiced_quantity_basic_u_m\", \"sum\")\n",
    "    )\n",
    "    \n",
    "    # Reindex to the full date range and fill missing values\n",
    "    monthly_grouped = monthly_grouped.reindex(date_range, fill_value=0)\n",
    "    \n",
    "    # Add customer number to each row\n",
    "    monthly_grouped[\"customer_number\"] = customer\n",
    "    \n",
    "    # Append to the monthly_data list\n",
    "    monthly_data.append(monthly_grouped.reset_index())\n",
    "\n",
    "# Concatenate the data for all customers\n",
    "final_time_series_df = pd.concat(monthly_data, ignore_index=True)\n",
    "\n",
    "print(final_time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf552731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>indicator_requested_delivery</th>\n",
       "      <th>sum_of_invoiced_quantity_basic_u_m</th>\n",
       "      <th>customer_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>182080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797227</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797228</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797229</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797230</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797231</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>239177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>797232 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index  indicator_requested_delivery  \\\n",
       "0      2018-02-28                             0   \n",
       "1      2018-03-31                             0   \n",
       "2      2018-04-30                             0   \n",
       "3      2018-05-31                             0   \n",
       "4      2018-06-30                             0   \n",
       "...           ...                           ...   \n",
       "797227 2023-05-31                             0   \n",
       "797228 2023-06-30                             0   \n",
       "797229 2023-07-31                             0   \n",
       "797230 2023-08-31                             0   \n",
       "797231 2023-09-30                             0   \n",
       "\n",
       "        sum_of_invoiced_quantity_basic_u_m customer_number  \n",
       "0                                      0.0          182080  \n",
       "1                                      0.0          182080  \n",
       "2                                      0.0          182080  \n",
       "3                                      0.0          182080  \n",
       "4                                      0.0          182080  \n",
       "...                                    ...             ...  \n",
       "797227                                 0.0          239177  \n",
       "797228                                 0.0          239177  \n",
       "797229                                 0.0          239177  \n",
       "797230                                 0.0          239177  \n",
       "797231                                 0.0          239177  \n",
       "\n",
       "[797232 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_time_series_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8b055",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e1f77",
   "metadata": {},
   "source": [
    "## missing values \n",
    "We dont have any missing values in our dataset, but if we had this should have been the first step for preprocessing the time series \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3a235",
   "metadata": {},
   "source": [
    "## transform the dates to period\n",
    "(not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb989a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_dataframe['requested_delivery_date_obdwdz_'] = pd.to_datetime(sales_dataframe['requested_delivery_date_obdwdz_']).dt.to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1961938",
   "metadata": {},
   "source": [
    "## Groupby the sales to get the total volume per customer per order date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfca3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_dataframe = final_time_series_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fa5398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_dataframe = sales_dataframe.groupby(['customer_number', \"index\"], as_index=False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8f311",
   "metadata": {},
   "source": [
    "# Feature engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eecc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_date_df = modelling_dataframe.groupby('customer_number', as_index=False)['requested_delivery_date_obdwdz_'].max()\n",
    "# max_date_df = max_date_df.rename(columns = {'requested_delivery_date_obdwdz_': \"max_order_date\"})\n",
    "# merged_df = pd.merge(modelling_dataframe , max_date_df, on = \"customer_number\")\n",
    "\n",
    "# # discover if the cusrtomers are active or not, e.g they made a purchase in the last 12 months\n",
    "# # we create a new column, if the customer is active we place 1 if not 0 \n",
    "# merged_df['cutoff_date'] = pd.Period('2022-03', 'M')\n",
    "# merged_df['is_after_cutoff'] = merged_df['max_order_date'] > merged_df['cutoff_date']\n",
    "# merged_df['is_after_cutoff'] = merged_df['is_after_cutoff'].astype(int)\n",
    "\n",
    "# # drop columns that we dont need\n",
    "# merged_df = merged_df.drop([\"cutoff_date\", \"max_order_date\"], axis=1)\n",
    "\n",
    "\n",
    "# # drop not active customers because we are only predicting for active customers \n",
    "# merged_df = merged_df[merged_df['is_after_cutoff'] == 1]\n",
    "\n",
    "# #drop the column after using it for filtering \n",
    "# merged_df = merged_df.drop('is_after_cutoff', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3168398",
   "metadata": {},
   "source": [
    "## Area as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d9855b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "area = pd.read_parquet(\"data_aggregated_per_customer_with_coords.parquet\")\n",
    "area = area[[\"customer_number\", \"area_categorical\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "236d7dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(modelling_dataframe, area, on = 'customer_number', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5c47ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9199a3",
   "metadata": {},
   "source": [
    "## Lags as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9e143d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way\n",
    "# for lag in range(2, 13):\n",
    "#     df[f'lagged_volume{lag}'] = df['invoiced_quantity_basic_u_m'].shift(lag)\n",
    "    \n",
    "# another way to do it \n",
    "for lag in range(2, 13):\n",
    "    df.loc[:, f'lagged_volume{lag}'] = df['sum_of_invoiced_quantity_basic_u_m'].shift(lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df037b",
   "metadata": {},
   "source": [
    "Using the  second piece of code that uses .loc for assignment is recommended. It not only avoids any potential issues but also makes the code more readable and adheres to the best practices of pandas DataFrame manipulation.\n",
    "\n",
    "Clarity and Readability: Using .loc makes the code more explicit and self-explanatory. It clearly shows that you are using label-based indexing to assign values to specific rows and columns in the DataFrame.\n",
    "\n",
    "Avoiding Warnings and Ambiguity: The first code snippet using direct assignment with square brackets and dot notation can trigger a warning about setting values on a copy of a slice from a DataFrame. While in this specific case it is unlikely to cause problems, it is best to use .loc to avoid any ambiguity and potential issues.\n",
    "\n",
    "Best Practice: Using .loc for setting values in pandas DataFrames is considered a best practice because it explicitly indicates that you are modifying the original DataFrame and ensures code safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a540ce",
   "metadata": {},
   "source": [
    "# Rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "84b9edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe_rolling = df.drop(columns=['area_categorical', 'lagged_volume2',\n",
    "       'lagged_volume3', 'lagged_volume4', 'lagged_volume5', 'lagged_volume6',\n",
    "       'lagged_volume7', 'lagged_volume8', 'lagged_volume9', 'lagged_volume10',\n",
    "       'lagged_volume11', 'lagged_volume12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2dfc4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fe_rolling_agg = {\n",
    "    \"sum_of_invoiced_quantity_basic_u_m\":\"mean\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d46d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_avg(\n",
    "    df,\n",
    "    date_col,\n",
    "    dict_features_to_roll,\n",
    "    window\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates rolling aggregation.\n",
    "    \"\"\"\n",
    "    df_grouped = df.groupby([\"customer_number\",date_col],as_index= False).agg(dict_features_to_roll)\n",
    "    for feature in dict_fe_rolling_agg:\n",
    "        df_grouped[f\"rolling_{window}_m_avg_{feature}\"] = df_grouped[feature].rolling(window=window).mean()\n",
    "        df_grouped[f\"rolling_{window}_m_std_{feature}\"] = df_grouped[feature].rolling(window=window).std()\n",
    "        df_grouped.drop(columns=feature, axis=1, inplace=True)\n",
    "        df_grouped.fillna(method=\"bfill\", inplace=True)\n",
    "    \n",
    "    return df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b156783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling averages\n",
    "# how long they have been active? \n",
    "# DO WE WANT THEM STATIC OR ROLLIONG ???\n",
    "# transformations scaling differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e5a31c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_number</th>\n",
       "      <th>index</th>\n",
       "      <th>indicator_requested_delivery</th>\n",
       "      <th>sum_of_invoiced_quantity_basic_u_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>156859</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>156859</td>\n",
       "      <td>2018-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>156859</td>\n",
       "      <td>2018-04-30</td>\n",
       "      <td>1</td>\n",
       "      <td>12600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>156859</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>156859</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751191</th>\n",
       "      <td>260392</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751192</th>\n",
       "      <td>260392</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751193</th>\n",
       "      <td>260392</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751194</th>\n",
       "      <td>260392</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751195</th>\n",
       "      <td>260392</td>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699516 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_number      index  indicator_requested_delivery  \\\n",
       "68              156859 2018-02-28                             0   \n",
       "69              156859 2018-03-31                             0   \n",
       "70              156859 2018-04-30                             1   \n",
       "71              156859 2018-05-31                             0   \n",
       "72              156859 2018-06-30                             0   \n",
       "...                ...        ...                           ...   \n",
       "751191          260392 2023-05-31                             0   \n",
       "751192          260392 2023-06-30                             0   \n",
       "751193          260392 2023-07-31                             0   \n",
       "751194          260392 2023-08-31                             0   \n",
       "751195          260392 2023-09-30                             0   \n",
       "\n",
       "        sum_of_invoiced_quantity_basic_u_m  \n",
       "68                                     0.0  \n",
       "69                                     0.0  \n",
       "70                                 12600.0  \n",
       "71                                     0.0  \n",
       "72                                     0.0  \n",
       "...                                    ...  \n",
       "751191                                 0.0  \n",
       "751192                                 0.0  \n",
       "751193                                 0.0  \n",
       "751194                                 0.0  \n",
       "751195                                 0.0  \n",
       "\n",
       "[699516 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fe_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c70e153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_average_df = get_rolling_avg(df_fe_rolling, \"index\", dict_fe_rolling_agg,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1530e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features = pd.merge(df, rolling_average_df, how='left', on=['customer_number','index' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e028f3",
   "metadata": {},
   "source": [
    "## Transform categorical to numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a88ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical variables\n",
    "df_with_features['area_categorical'] = pd.Categorical(df_with_features['area_categorical']).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af1686",
   "metadata": {},
   "source": [
    "# Split the dataset into input features and the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "833ed617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['customer_number', 'index', 'indicator_requested_delivery',\n",
       "       'sum_of_invoiced_quantity_basic_u_m', 'area_categorical',\n",
       "       'lagged_volume2', 'lagged_volume3', 'lagged_volume4', 'lagged_volume5',\n",
       "       'lagged_volume6', 'lagged_volume7', 'lagged_volume8', 'lagged_volume9',\n",
       "       'lagged_volume10', 'lagged_volume11', 'lagged_volume12',\n",
       "       'rolling_3_m_avg_sum_of_invoiced_quantity_basic_u_m',\n",
       "       'rolling_3_m_std_sum_of_invoiced_quantity_basic_u_m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eab55abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_with_features.drop('indicator_requested_delivery', axis=1)\n",
    "y = df_with_features['indicator_requested_delivery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c357c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e08f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a47ce0b2",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f0dd805b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: customer_number, index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [101]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m     45\u001b[0m }\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Train the LGBM model\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     51\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2603\u001b[0m     )\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1474\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mpandas_categorical\n\u001b[0;32m   1473\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[1;32m-> 1474\u001b[0m data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1475\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1476\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1477\u001b[0m \u001b[43m                                                                                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1478\u001b[0m label \u001b[38;5;241m=\u001b[39m _label_from_pandas(label)\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;66;03m# process for args\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:594\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bad_indices:\n\u001b[0;32m    593\u001b[0m     bad_index_cols_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data\u001b[38;5;241m.\u001b[39mcolumns[bad_indices])\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.dtypes for data must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not expect the data types in the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad_index_cols_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    597\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in the following fields: customer_number, index"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocess the data\n",
    "df['index'] = pd.to_datetime(df['index']).dt.to_period('M').apply(lambda r: pd.Timestamp(r.to_timestamp()))\n",
    "df.sort_values('index', inplace=True)\n",
    "df['index'] = pd.to_datetime(df['index']).dt.to_period('M').apply(lambda r: pd.Timestamp(r.to_timestamp()))\n",
    "\n",
    "# Fill missing values in lagged volume features\n",
    "lagged_features = ['lagged_volume2', 'lagged_volume3', 'lagged_volume4', 'lagged_volume5', 'lagged_volume6',\n",
    "                   'lagged_volume7', 'lagged_volume8', 'lagged_volume9', 'lagged_volume10', 'lagged_volume11',\n",
    "                   'lagged_volume12']\n",
    "df[lagged_features] = df[lagged_features].fillna(0)\n",
    "\n",
    "# Perform one-hot encoding for categorical columns\n",
    "categorical_columns = ['area_categorical']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
    "\n",
    "# Split the DataFrame into features (X) and target variable (y)\n",
    "X = df_encoded.drop('indicator_requested_delivery', axis=1)\n",
    "y = df_encoded['indicator_requested_delivery']\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Iterate over time series cross-validation splits\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Convert the DataFrame into a LightGBM dataset format\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Define the LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "    # Train the LGBM model\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using mean squared error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print('Mean Squared Error:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6568db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocess the data\n",
    "df.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "df['requested_delivery_date_obdwdz__x'] = pd.to_datetime(df['requested_delivery_date_obdwdz__x']).dt.to_timestamp()\n",
    "df.sort_values('requested_delivery_date_obdwdz__x', inplace=True)\n",
    "df['requested_delivery_date_obdwdz__y'] = pd.to_datetime(df['requested_delivery_date_obdwdz__y']).dt.to_timestamp()\n",
    "\n",
    "# Fill missing values in lagged volume features\n",
    "lagged_features = ['lagged_volume2', 'lagged_volume3', 'lagged_volume4', 'lagged_volume5', 'lagged_volume6',\n",
    "                   'lagged_volume7', 'lagged_volume8', 'lagged_volume9', 'lagged_volume10', 'lagged_volume11',\n",
    "                   'lagged_volume12']\n",
    "df[lagged_features] = df[lagged_features].fillna(0)\n",
    "\n",
    "# Perform one-hot encoding for categorical columns\n",
    "categorical_columns = ['area_categorical']\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[categorical_columns]))\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_columns)\n",
    "df_encoded = pd.concat([df, encoded_cols], axis=1).drop(categorical_columns, axis=1)\n",
    "\n",
    "# Convert the customer_number column to float\n",
    "df_encoded['customer_number'] = pd.to_numeric(df_encoded['customer_number'], errors='coerce')\n",
    "\n",
    "# Split the DataFrame into features (X) and target variable (y)\n",
    "X = df_encoded.drop('invoiced_quantity_basic_u_m', axis=1)\n",
    "y = df_encoded['invoiced_quantity_basic_u_m']\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Iterate over time series cross-validation splits\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Convert the DataFrame into a LightGBM dataset format\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "    # Define the LightGBM parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mse',\n",
    "        'num_leaves': 50,\n",
    "        'learning_rate': 0.1,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "    # Train the LGBM model\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model using mean squared error (MSE)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print('Mean Squared Error:', mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ld",
   "language": "python",
   "name": "env_ld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
